{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.chat import Chat\n",
    "from app.models.UserInput import UserInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongsakon/Study/CEDT/2_semeter_1/data_sci/dsde-project/src/database/qdrant.py:37: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.df = pd.read_csv(\"combined_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "chat = Chat()\n",
    "chat.init_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"I want to know paper about attention is all you need anything paper about transformer in computer science\"\n",
    "\n",
    "user_input = UserInput(\n",
    "    chat_id=\"abc\",\n",
    "    message=user_message,\n",
    "    currentGraph=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template_prompt = \"\"\"You are an AI assistance for encouraging the system to initialize new \"Node\" from below data that was fetched from QdrantDB. Node have two types which are \"Paper\" and \"Keyword\".\n",
    "Keyword is a short word that related to topic, title or field of the input messages from user while \"Paper\" is the main content of articles in the database. Your tasks are generating \"Keyword\" node based on summarizing the content of user messages and \"Paper\" node, a node that contains more . Both of them follow this structure of node object: \n",
    "\n",
    "Don't forget to ensure that the values that will be assigned are related to data in the database.\n",
    "\"\"\"\n",
    "chat.set_system_template_prompt(system_template_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Chat Template Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template_prompt = \"\"\"\n",
    "{paper_data}\n",
    "\"\"\"\n",
    "chat.set_chat_template_prompt(chat_template_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\n",
      "\n",
      "                Title: On Explaining with Attention Matrices\n",
      "                Year: 2024\n",
      "                Authors: Omar Naim and Nicholas Asher\n",
      "                Source: arxiv\n",
      "                Abstract:   This paper explores the much discussed, possible explanatory link between\n",
      "attention weights (AW) in transformer models and predicted output. Contrary to\n",
      "intuition and early research on attention, more recent prior research has\n",
      "provided formal arguments and empirical evidence that AW are not explanatorily\n",
      "relevant. We show that the formal arguments are incorrect. We introduce and\n",
      "effectively compute efficient attention, which isolates the effective\n",
      "components of attention matrices in tasks and models in which AW play an\n",
      "explanatory role. We show that efficient attention has a causal role (provides\n",
      "minimally necessary and sufficient conditions) for predicting model output in\n",
      "NLP tasks requiring contextual information, and we show, contrary to [7], that\n",
      "efficient attention matrices are probability distributions and are effectively\n",
      "calculable. Thus, they should play an important part in the explanation of\n",
      "attention based model behavior. We offer empirical experiments in support of\n",
      "our method illustrating various properties of efficient attention with various\n",
      "metrics on four datasets.\n",
      "\n",
      "            \n",
      "                Title: Attention Mechanism in Neural Networks: Where it Comes and Where it Goes\n",
      "                Year: 2022\n",
      "                Authors: Derya Soydaner\n",
      "                Source: arxiv\n",
      "                Abstract:   A long time ago in the machine learning literature, the idea of incorporating\n",
      "a mechanism inspired by the human visual system into neural networks was\n",
      "introduced. This idea is named the attention mechanism, and it has gone through\n",
      "a long development period. Today, many works have been devoted to this idea in\n",
      "a variety of tasks. Remarkable performance has recently been demonstrated. The\n",
      "goal of this paper is to provide an overview from the early work on searching\n",
      "for ways to implement attention idea with neural networks until the recent\n",
      "trends. This review emphasizes the important milestones during this progress\n",
      "regarding different tasks. By this way, this study aims to provide a road map\n",
      "for researchers to explore the current development and get inspired for novel\n",
      "approaches beyond the attention.\n",
      "\n",
      "            \n",
      "                Title: Parallel Attention Mechanisms in Neural Machine Translation\n",
      "                Year: 2018\n",
      "                Authors: Julian Richard Medina,  Jugal Kalita\n",
      "                Source: arxiv\n",
      "                Abstract:   Recent papers in neural machine translation have proposed the strict use of\n",
      "attention mechanisms over previous standards such as recurrent and\n",
      "convolutional neural networks (RNNs and CNNs). We propose that by running\n",
      "traditionally stacked encoding branches from encoder-decoder attention- focused\n",
      "architectures in parallel, that even more sequential operations can be removed\n",
      "from the model and thereby decrease training time. In particular, we modify the\n",
      "recently published attention-based architecture called Transformer by Google,\n",
      "by replacing sequential attention modules with parallel ones, reducing the\n",
      "amount of training time and substantially improving BLEU scores at the same\n",
      "time. Experiments over the English to German and English to French translation\n",
      "tasks show that our model establishes a new state of the art.\n",
      "\n",
      "            \n",
      "                Title: T-former: An Efficient Transformer for Image Inpainting\n",
      "                Year: 2023\n",
      "                Authors: Ye Deng,  Siqi Hui,  Sanping Zhou,  Deyu Meng,  Jinjun Wang\n",
      "                Source: arxiv\n",
      "                Abstract:   Benefiting from powerful convolutional neural networks (CNNs), learning-based\n",
      "image inpainting methods have made significant breakthroughs over the years.\n",
      "However, some nature of CNNs (e.g. local prior, spatially shared parameters)\n",
      "limit the performance in the face of broken images with diverse and complex\n",
      "forms. Recently, a class of attention-based network architectures, called\n",
      "transformer, has shown significant performance on natural language processing\n",
      "fields and high-level vision tasks. Compared with CNNs, attention operators are\n",
      "better at long-range modeling and have dynamic weights, but their computational\n",
      "complexity is quadratic in spatial resolution, and thus less suitable for\n",
      "applications involving higher resolution images, such as image inpainting. In\n",
      "this paper, we design a novel attention linearly related to the resolution\n",
      "according to Taylor expansion. And based on this attention, a network called\n",
      "$T$-former is designed for image inpainting. Experiments on several benchmark\n",
      "datasets demonstrate that our proposed method achieves state-of-the-art\n",
      "accuracy while maintaining a relatively low number of parameters and\n",
      "computational complexity. The code can be found at\n",
      "\\href{https://github.com/dengyecode/T-former_image_inpainting}{github.com/dengyecode/T-former\\_image\\_inpainting}\n",
      "\n",
      "            \n",
      "                Title: A General Survey on Attention Mechanisms in Deep Learning\n",
      "                Year: 2022\n",
      "                Authors: Gianni Brauwers and Flavius Frasincar\n",
      "                Source: arxiv\n",
      "                Abstract:   Attention is an important mechanism that can be employed for a variety of\n",
      "deep learning models across many different domains and tasks. This survey\n",
      "provides an overview of the most important attention mechanisms proposed in the\n",
      "literature. The various attention mechanisms are explained by means of a\n",
      "framework consisting of a general attention model, uniform notation, and a\n",
      "comprehensive taxonomy of attention mechanisms. Furthermore, the various\n",
      "measures for evaluating attention models are reviewed, and methods to\n",
      "characterize the structure of attention models based on the proposed framework\n",
      "are discussed. Last, future work in the field of attention models is\n",
      "considered.\n",
      "\n",
      "            \n",
      "                Title: Energy Transformer\n",
      "                Year: 2023\n",
      "                Authors: Benjamin Hoover,  Yuchen Liang,  Bao Pham,  Rameswar Panda,  Hendrik\n",
      "  Strobelt,  Duen Horng Chau,  Mohammed J. Zaki,  Dmitry Krotov\n",
      "                Source: arxiv\n",
      "                Abstract:   Our work combines aspects of three promising paradigms in machine learning,\n",
      "namely, attention mechanism, energy-based models, and associative memory.\n",
      "Attention is the power-house driving modern deep learning successes, but it\n",
      "lacks clear theoretical foundations. Energy-based models allow a principled\n",
      "approach to discriminative and generative tasks, but the design of the energy\n",
      "functional is not straightforward. At the same time, Dense Associative Memory\n",
      "models or Modern Hopfield Networks have a well-established theoretical\n",
      "foundation, and allow an intuitive design of the energy function. We propose a\n",
      "novel architecture, called the Energy Transformer (or ET for short), that uses\n",
      "a sequence of attention layers that are purposely designed to minimize a\n",
      "specifically engineered energy function, which is responsible for representing\n",
      "the relationships between the tokens. In this work, we introduce the\n",
      "theoretical foundations of ET, explore its empirical capabilities using the\n",
      "image completion task, and obtain strong quantitative results on the graph\n",
      "anomaly detection and graph classification tasks.\n",
      "\n",
      "            \n",
      "                Title: Attention in Natural Language Processing\n",
      "                Year: 2021\n",
      "                Authors: Andrea Galassi,  Marco Lippi,  Paolo Torroni\n",
      "                Source: arxiv\n",
      "                Abstract:   Attention is an increasingly popular mechanism used in a wide range of neural\n",
      "architectures. The mechanism itself has been realized in a variety of formats.\n",
      "However, because of the fast-paced advances in this domain, a systematic\n",
      "overview of attention is still missing. In this article, we define a unified\n",
      "model for attention architectures in natural language processing, with a focus\n",
      "on those designed to work with vector representations of the textual data. We\n",
      "propose a taxonomy of attention models according to four dimensions: the\n",
      "representation of the input, the compatibility function, the distribution\n",
      "function, and the multiplicity of the input and/or output. We present the\n",
      "examples of how prior information can be exploited in attention models and\n",
      "discuss ongoing research efforts and open challenges in the area, providing the\n",
      "first extensive categorization of the vast body of literature in this exciting\n",
      "domain.\n",
      "\n",
      "            \n",
      "                Title: A Theoretical Understanding of Shallow Vision Transformers: Learning,\n",
      "  Generalization, and Sample Complexity\n",
      "                Year: 2023\n",
      "                Authors: Hongkang Li,  Meng Wang,  Sijia Liu,  Pin-yu Chen\n",
      "                Source: arxiv\n",
      "                Abstract:   Vision Transformers (ViTs) with self-attention modules have recently achieved\n",
      "great empirical success in many vision tasks. Due to non-convex interactions\n",
      "across layers, however, theoretical learning and generalization analysis is\n",
      "mostly elusive. Based on a data model characterizing both label-relevant and\n",
      "label-irrelevant tokens, this paper provides the first theoretical analysis of\n",
      "training a shallow ViT, i.e., one self-attention layer followed by a two-layer\n",
      "perceptron, for a classification task. We characterize the sample complexity to\n",
      "achieve a zero generalization error. Our sample complexity bound is positively\n",
      "correlated with the inverse of the fraction of label-relevant tokens, the token\n",
      "noise level, and the initial model error. We also prove that a training process\n",
      "using stochastic gradient descent (SGD) leads to a sparse attention map, which\n",
      "is a formal verification of the general intuition about the success of\n",
      "attention. Moreover, this paper indicates that a proper token sparsification\n",
      "can improve the test performance by removing label-irrelevant and/or noisy\n",
      "tokens, including spurious correlations. Empirical experiments on synthetic\n",
      "data and CIFAR-10 dataset justify our theoretical results and generalize to\n",
      "deeper ViTs.\n",
      "\n",
      "            \n",
      "                Title: Unsupervised Extractive Summarization by Pre-training Hierarchical\n",
      "  Transformers\n",
      "                Year: 2021\n",
      "                Authors: Shusheng Xu,  Xingxing Zhang,  Yi Wu,  Furu Wei and Ming Zhou\n",
      "                Source: arxiv\n",
      "                Abstract:   Unsupervised extractive document summarization aims to select important\n",
      "sentences from a document without using labeled summaries during training.\n",
      "Existing methods are mostly graph-based with sentences as nodes and edge\n",
      "weights measured by sentence similarities. In this work, we find that\n",
      "transformer attentions can be used to rank sentences for unsupervised\n",
      "extractive summarization. Specifically, we first pre-train a hierarchical\n",
      "transformer model using unlabeled documents only. Then we propose a method to\n",
      "rank sentences using sentence-level self-attentions and pre-training\n",
      "objectives. Experiments on CNN/DailyMail and New York Times datasets show our\n",
      "model achieves state-of-the-art performance on unsupervised summarization. We\n",
      "also find in experiments that our model is less dependent on sentence\n",
      "positions. When using a linear combination of our model and a recent\n",
      "unsupervised model explicitly modeling sentence positions, we obtain even\n",
      "better results.\n",
      "\n",
      "            \n",
      "                Title: Attention Can Reflect Syntactic Structure (If You Let It)\n",
      "                Year: 2021\n",
      "                Authors: Vinit Ravishankar,  Artur Kulmizev,  Mostafa Abdou,  Anders S{\\o}gaard, \n",
      "  Joakim Nivre\n",
      "                Source: arxiv\n",
      "                Abstract:   Since the popularization of the Transformer as a general-purpose feature\n",
      "encoder for NLP, many studies have attempted to decode linguistic structure\n",
      "from its novel multi-head attention mechanism. However, much of such work\n",
      "focused almost exclusively on English -- a language with rigid word order and a\n",
      "lack of inflectional morphology. In this study, we present decoding experiments\n",
      "for multilingual BERT across 18 languages in order to test the generalizability\n",
      "of the claim that dependency syntax is reflected in attention patterns. We show\n",
      "that full trees can be decoded above baseline accuracy from single attention\n",
      "heads, and that individual relations are often tracked by the same heads across\n",
      "languages. Furthermore, in an attempt to address recent debates about the\n",
      "status of attention as an explanatory mechanism, we experiment with fine-tuning\n",
      "mBERT on a supervised parsing objective while freezing different series of\n",
      "parameters. Interestingly, in steering the objective to learn explicit\n",
      "linguistic structure, we find much of the same structure represented in the\n",
      "resulting attention patterns, with interesting differences with respect to\n",
      "which parameters are frozen.\n",
      "\n",
      "            \n",
      "\n",
      "None\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"id\": \"keyword_attention_mechanism\",\n",
      "        \"type\": \"Keyword\",\n",
      "        \"data\": {\n",
      "            \"keyword\": \"attention mechanism\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"keyword_transformer_models\",\n",
      "        \"type\": \"Keyword\",\n",
      "        \"data\": {\n",
      "            \"keyword\": \"transformer models\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"keyword_natural_language_processing\",\n",
      "        \"type\": \"Keyword\",\n",
      "        \"data\": {\n",
      "            \"keyword\": \"natural language processing\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"keyword_neural_machine_translation\",\n",
      "        \"type\": \"Keyword\",\n",
      "        \"data\": {\n",
      "            \"keyword\": \"neural machine translation\"\n",
      "        }\n",
      "    },\n",
      "\n",
      "    {\n",
      "        \"id\": \"paper_on_explaining_with_attention_matrices\",\n",
      "        \"type\": \"Paper\",\n",
      "        \"data\": {\n",
      "            \"title\": \"On Explaining with Attention Matrices\",\n",
      "            \"year\": 2024,\n",
      "            \"authors\": \"Omar Naim and Nicholas Asher\",\n",
      "            \"source\": \"arxiv\",\n",
      "            \"abstract\": \"Explores the explanatory link between attention weights and predicted output in transformer models, arguing against prior claims of irrelevance and introducing 'efficient attention' for causal explanation.\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"paper_attention_mechanism_in_neural_networks\",\n",
      "        \"type\": \"Paper\",\n",
      "        \"data\": {\n",
      "            \"title\": \"Attention Mechanism in Neural Networks: Where it Comes and Where it Goes\",\n",
      "            \"year\": 2022,\n",
      "            \"authors\": \"Derya Soydaner\",\n",
      "            \"source\": \"arxiv\",\n",
      "            \"abstract\": \"Provides an overview of the development of attention mechanisms in neural networks, from early work to recent trends, offering a roadmap for researchers.\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"paper_parallel_attention_mechanisms\",\n",
      "        \"type\": \"Paper\",\n",
      "        \"data\": {\n",
      "            \"title\": \"Parallel Attention Mechanisms in Neural Machine Translation\",\n",
      "            \"year\": 2018,\n",
      "            \"authors\": \"Julian Richard Medina, Jugal Kalita\",\n",
      "            \"source\": \"arxiv\",\n",
      "            \"abstract\": \"Proposes using parallel attention mechanisms in neural machine translation to reduce training time and improve BLEU scores, modifying the Transformer architecture.\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"paper_t_former\",\n",
      "        \"type\": \"Paper\",\n",
      "        \"data\": {\n",
      "            \"title\": \"T-former: An Efficient Transformer for Image Inpainting\",\n",
      "            \"year\": 2023,\n",
      "            \"authors\": \"Ye Deng, Siqi Hui, Sanping Zhou, Deyu Meng, Jinjun Wang\",\n",
      "            \"source\": \"arxiv\",\n",
      "            \"abstract\": \"Introduces T-former, an efficient transformer network for image inpainting, using attention linearly related to resolution for improved accuracy and lower computational complexity.\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"paper_general_survey_on_attention\",\n",
      "        \"type\": \"Paper\",\n",
      "        \"data\": {\n",
      "            \"title\": \"A General Survey on Attention Mechanisms in Deep Learning\",\n",
      "            \"year\": 2022,\n",
      "            \"authors\": \"Gianni Brauwers and Flavius Frasincar\",\n",
      "            \"source\": \"arxiv\",\n",
      "            \"abstract\": \"Offers a comprehensive overview of attention mechanisms in deep learning, using a unified framework, notation, and taxonomy, and discussing evaluation measures and future directions.\"\n",
      "        }\n",
      "    },\n",
      "        {\n",
      "        \"id\": \"paper_energy_transformer\",\n",
      "        \"type\": \"Paper\",\n",
      "        \"data\": {\n",
      "            \"title\": \"Energy Transformer\",\n",
      "            \"year\": 2023,\n",
      "            \"authors\": \"Benjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik Strobelt, Duen Horng Chau, Mohammed J. Zaki, Dmitry Krotov\",\n",
      "            \"source\": \"arxiv\",\n",
      "            \"abstract\": \"Combines attention mechanisms, energy-based models, and associative memory to propose the Energy Transformer (ET), which uses attention layers to minimize an energy function representing token relationships.\"\n",
      "        }\n",
      "    },\n",
      "{\n",
      "        \"id\": \"paper_attention_in_natural_language_processing\",\n",
      "        \"type\": \"Paper\",\n",
      "        \"data\": {\n",
      "            \"title\": \"Attention in Natural Language Processing\",\n",
      "            \"year\": 2021,\n",
      "            \"authors\": \"Andrea Galassi, Marco Lippi, Paolo Torroni\",\n",
      "            \"source\": \"arxiv\",\n",
      "            \"abstract\": \"Provides a unified model and taxonomy for attention architectures in NLP, focusing on vector representations of text data and categorizing models based on input representation, compatibility function, distribution function, and input/output multiplicity.\"\n",
      "        }\n",
      "    },\n",
      "{\n",
      "        \"id\": \"paper_theoretical_understanding_of_shallow_vision_transformers\",\n",
      "        \"type\": \"Paper\",\n",
      "        \"data\": {\n",
      "            \"title\": \"A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity\",\n",
      "            \"year\": 2023,\n",
      "            \"authors\": \"Hongkang Li, Meng Wang, Sijia Liu, Pin-yu Chen\",\n",
      "            \"source\": \"arxiv\",\n",
      "            \"abstract\": \"Presents the first theoretical analysis of training a shallow Vision Transformer for classification, characterizing sample complexity and proving that SGD leads to a sparse attention map.\"\n",
      "        }\n",
      "    },\n",
      "{\n",
      "        \"id\": \"paper_unsupervised_extractive_summarization\",\n",
      "        \"type\": \"Paper\",\n",
      "        \"data\": {\n",
      "            \"title\": \"Unsupervised Extractive Summarization by Pre-training Hierarchical Transformers\",\n",
      "            \"year\": 2021,\n",
      "            \"authors\": \"Shusheng Xu, Xingxing Zhang, Yi Wu, Furu Wei and Ming Zhou\",\n",
      "            \"source\": \"arxiv\",\n",
      "            \"abstract\": \"Proposes using pre-trained hierarchical transformers for unsupervised extractive summarization, ranking sentences using self-attentions and pre-training objectives.\"\n",
      "        }\n",
      "    },\n",
      "{\n",
      "        \"id\": \"paper_attention_can_reflect_syntactic_structure\",\n",
      "        \"type\": \"Paper\",\n",
      "        \"data\": {\n",
      "            \"title\": \"Attention Can Reflect Syntactic Structure (If You Let It)\",\n",
      "            \"year\": 2021,\n",
      "            \"authors\": \"Vinit Ravishankar, Artur Kulmizev, Mostafa Abdou, Anders SÃ¸gaard, Joakim Nivre\",\n",
      "            \"source\": \"arxiv\",\n",
      "            \"abstract\": \"Investigates the reflection of dependency syntax in attention patterns of multilingual BERT across 18 languages, showing that syntactic structure can be decoded from attention and is further represented when fine-tuning on parsing tasks.\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "```\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "chat.test_initial_chat(user_input=user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Detect Additional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_additional_data_template = \"\"\"\n",
    "{user_message}\n",
    "\n",
    "{current_graph}\n",
    "\"\"\"\n",
    "chat.set_detect_additional_data_template(detect_additional_data_template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsde-cedt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
