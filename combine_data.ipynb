{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/dsde-cedt/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('BAAI/bge-m3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\n",
    "sentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n",
    "               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03411698, -0.04707836, -0.00089452, ...,  0.04828522,\n",
       "         0.0075543 , -0.02961659],\n",
       "       [-0.0104174 , -0.0447926 , -0.02429202, ..., -0.00819299,\n",
       "         0.01503989,  0.01113798]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = embedding_model.encode(sentence_1)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.database.qdrant import QdrantVectorDB\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.database.qdrant.QdrantVectorDB at 0x3c74dabd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "qdrant = QdrantVectorDB(url=url, api_key=api_key, embedding_model=embedding_model)\n",
    "qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Scopus and Arxiv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r2/133yy5hj2g7fq987s7zh0_q40000gn/T/ipykernel_76675/3421694929.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  arxiv_df = pd.read_csv(\"./cleaned_arxiv.csv\")\n"
     ]
    }
   ],
   "source": [
    "scopus_df = pd.read_csv(\"./scopus_unexplode_data.csv\")\n",
    "arxiv_df = pd.read_csv(\"./cleaned_arxiv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>references</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sureerat Thuekeaw, Kris Angkanaporn, Chackrit ...</td>\n",
       "      <td>2022</td>\n",
       "      <td>85131139456</td>\n",
       "      <td>Microencapsulated basil oil (Ocimum basilicum ...</td>\n",
       "      <td>Objective: Microencapsulation is a technique t...</td>\n",
       "      <td>['85039040394', '85050697915', '84920164411', ...</td>\n",
       "      <td>[('Food Science', '1106', 'AGRI'), ('Physiolog...</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abhijit V. Lele, Sarah Wahlster, Bhunyawee Alu...</td>\n",
       "      <td>2022</td>\n",
       "      <td>85121351780</td>\n",
       "      <td>Perceptions Regarding the SARS-CoV-2 Pandemic'...</td>\n",
       "      <td>Background: The SARS-CoV-2 (COVID-19) pandemic...</td>\n",
       "      <td>['85104589379', '85083241171', '85078262578', ...</td>\n",
       "      <td>[('Surgery', '2746', 'MEDI'), ('Neurology (cli...</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Solaphat Hemrungrojn, Arisara Amrapala, Michae...</td>\n",
       "      <td>2022</td>\n",
       "      <td>85131660961</td>\n",
       "      <td>Construction of a short version of the Montrea...</td>\n",
       "      <td>Background: The Montreal Cognitive Assessment ...</td>\n",
       "      <td>['84982975791', '84871671961', '85097597113', ...</td>\n",
       "      <td>[('Neuroscience (all)', '2800', 'NEUR')]</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Erik Johansson, Ferenc Tasnádi, Annop Ektarawo...</td>\n",
       "      <td>2022</td>\n",
       "      <td>85124670542</td>\n",
       "      <td>The effect of strain and pressure on the elect...</td>\n",
       "      <td>Different theoretical methodologies are employ...</td>\n",
       "      <td>['0035282206', '0035508561', '0035858409', '18...</td>\n",
       "      <td>[('Physics and Astronomy (all)', '3100', 'PHYS')]</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kunanya Masodsai, Rungchai Chaunchaiyakul,</td>\n",
       "      <td>2022</td>\n",
       "      <td>85143878806</td>\n",
       "      <td>Dynamic Cardiopulmonary and Metabolic Function...</td>\n",
       "      <td>The purpose of this study was to investigate a...</td>\n",
       "      <td>['25444452457', '49949090130', '84860884417', ...</td>\n",
       "      <td>[('Physiology (medical)', '2737', 'MEDI')]</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             authors  year           id  \\\n",
       "0  Sureerat Thuekeaw, Kris Angkanaporn, Chackrit ...  2022  85131139456   \n",
       "1  Abhijit V. Lele, Sarah Wahlster, Bhunyawee Alu...  2022  85121351780   \n",
       "2  Solaphat Hemrungrojn, Arisara Amrapala, Michae...  2022  85131660961   \n",
       "3  Erik Johansson, Ferenc Tasnádi, Annop Ektarawo...  2022  85124670542   \n",
       "4        Kunanya Masodsai, Rungchai Chaunchaiyakul,   2022  85143878806   \n",
       "\n",
       "                                               title  \\\n",
       "0  Microencapsulated basil oil (Ocimum basilicum ...   \n",
       "1  Perceptions Regarding the SARS-CoV-2 Pandemic'...   \n",
       "2  Construction of a short version of the Montrea...   \n",
       "3  The effect of strain and pressure on the elect...   \n",
       "4  Dynamic Cardiopulmonary and Metabolic Function...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Objective: Microencapsulation is a technique t...   \n",
       "1  Background: The SARS-CoV-2 (COVID-19) pandemic...   \n",
       "2  Background: The Montreal Cognitive Assessment ...   \n",
       "3  Different theoretical methodologies are employ...   \n",
       "4  The purpose of this study was to investigate a...   \n",
       "\n",
       "                                          references  \\\n",
       "0  ['85039040394', '85050697915', '84920164411', ...   \n",
       "1  ['85104589379', '85083241171', '85078262578', ...   \n",
       "2  ['84982975791', '84871671961', '85097597113', ...   \n",
       "3  ['0035282206', '0035508561', '0035858409', '18...   \n",
       "4  ['25444452457', '49949090130', '84860884417', ...   \n",
       "\n",
       "                                            category  source  \n",
       "0  [('Food Science', '1106', 'AGRI'), ('Physiolog...  scopus  \n",
       "1  [('Surgery', '2746', 'MEDI'), ('Neurology (cli...  scopus  \n",
       "2           [('Neuroscience (all)', '2800', 'NEUR')]  scopus  \n",
       "3  [('Physics and Astronomy (all)', '3100', 'PHYS')]  scopus  \n",
       "4         [('Physiology (medical)', '2737', 'MEDI')]  scopus  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scopus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>references</th>\n",
       "      <th>category</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0001</td>\n",
       "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>Phys.Rev.D76:013009,2007</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>2008</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0005</td>\n",
       "      <td>Wael Abu-Shammala and Alberto Torchinsky</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>Illinois J. Math. 52 (2008) no.2, 681-689</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>2013</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704.0007</td>\n",
       "      <td>Alejandro Corichi, Tatjana Vukasinac and Jose ...</td>\n",
       "      <td>Polymer Quantum Mechanics and its Continuum Limit</td>\n",
       "      <td>Phys.Rev.D76:044016,2007</td>\n",
       "      <td>gr-qc</td>\n",
       "      <td>A rather non-standard quantum representation...</td>\n",
       "      <td>2008</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704.0008</td>\n",
       "      <td>Damian C. Swift</td>\n",
       "      <td>Numerical solution of shock and ramp compressi...</td>\n",
       "      <td>Journal of Applied Physics, vol 104, 073536 (2...</td>\n",
       "      <td>cond-mat.mtrl-sci</td>\n",
       "      <td>A general formulation was developed to repre...</td>\n",
       "      <td>2009</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704.0009</td>\n",
       "      <td>Paul Harvey, Bruno Merin, Tracy L. Huard, Luis...</td>\n",
       "      <td>The Spitzer c2d Survey of Large, Nearby, Inste...</td>\n",
       "      <td>Astrophys.J.663:1149-1173,2007</td>\n",
       "      <td>astro-ph</td>\n",
       "      <td>We discuss the results from the combined IRA...</td>\n",
       "      <td>2010</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                            authors  \\\n",
       "0  704.0001  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...   \n",
       "1  704.0005           Wael Abu-Shammala and Alberto Torchinsky   \n",
       "2  704.0007  Alejandro Corichi, Tatjana Vukasinac and Jose ...   \n",
       "3  704.0008                                    Damian C. Swift   \n",
       "4  704.0009  Paul Harvey, Bruno Merin, Tracy L. Huard, Luis...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Calculation of prompt diphoton production cros...   \n",
       "1  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "2  Polymer Quantum Mechanics and its Continuum Limit   \n",
       "3  Numerical solution of shock and ramp compressi...   \n",
       "4  The Spitzer c2d Survey of Large, Nearby, Inste...   \n",
       "\n",
       "                                          references           category  \\\n",
       "0                           Phys.Rev.D76:013009,2007             hep-ph   \n",
       "1          Illinois J. Math. 52 (2008) no.2, 681-689    math.CA math.FA   \n",
       "2                           Phys.Rev.D76:044016,2007              gr-qc   \n",
       "3  Journal of Applied Physics, vol 104, 073536 (2...  cond-mat.mtrl-sci   \n",
       "4                     Astrophys.J.663:1149-1173,2007           astro-ph   \n",
       "\n",
       "                                            abstract  year source  \n",
       "0    A fully differential calculation in perturba...  2008  arxiv  \n",
       "1    In this paper we show how to compute the $\\L...  2013  arxiv  \n",
       "2    A rather non-standard quantum representation...  2008  arxiv  \n",
       "3    A general formulation was developed to repre...  2009  arxiv  \n",
       "4    We discuss the results from the combined IRA...  2010  arxiv  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>year</th>\n",
       "      <th>source</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85131139456</td>\n",
       "      <td>Microencapsulated basil oil (Ocimum basilicum ...</td>\n",
       "      <td>Objective: Microencapsulation is a technique t...</td>\n",
       "      <td>Sureerat Thuekeaw, Kris Angkanaporn, Chackrit ...</td>\n",
       "      <td>[('Food Science', '1106', 'AGRI'), ('Physiolog...</td>\n",
       "      <td>2022</td>\n",
       "      <td>scopus</td>\n",
       "      <td>['85039040394', '85050697915', '84920164411', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85121351780</td>\n",
       "      <td>Perceptions Regarding the SARS-CoV-2 Pandemic'...</td>\n",
       "      <td>Background: The SARS-CoV-2 (COVID-19) pandemic...</td>\n",
       "      <td>Abhijit V. Lele, Sarah Wahlster, Bhunyawee Alu...</td>\n",
       "      <td>[('Surgery', '2746', 'MEDI'), ('Neurology (cli...</td>\n",
       "      <td>2022</td>\n",
       "      <td>scopus</td>\n",
       "      <td>['85104589379', '85083241171', '85078262578', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85131660961</td>\n",
       "      <td>Construction of a short version of the Montrea...</td>\n",
       "      <td>Background: The Montreal Cognitive Assessment ...</td>\n",
       "      <td>Solaphat Hemrungrojn, Arisara Amrapala, Michae...</td>\n",
       "      <td>[('Neuroscience (all)', '2800', 'NEUR')]</td>\n",
       "      <td>2022</td>\n",
       "      <td>scopus</td>\n",
       "      <td>['84982975791', '84871671961', '85097597113', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85124670542</td>\n",
       "      <td>The effect of strain and pressure on the elect...</td>\n",
       "      <td>Different theoretical methodologies are employ...</td>\n",
       "      <td>Erik Johansson, Ferenc Tasnádi, Annop Ektarawo...</td>\n",
       "      <td>[('Physics and Astronomy (all)', '3100', 'PHYS')]</td>\n",
       "      <td>2022</td>\n",
       "      <td>scopus</td>\n",
       "      <td>['0035282206', '0035508561', '0035858409', '18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85143878806</td>\n",
       "      <td>Dynamic Cardiopulmonary and Metabolic Function...</td>\n",
       "      <td>The purpose of this study was to investigate a...</td>\n",
       "      <td>Kunanya Masodsai, Rungchai Chaunchaiyakul,</td>\n",
       "      <td>[('Physiology (medical)', '2737', 'MEDI')]</td>\n",
       "      <td>2022</td>\n",
       "      <td>scopus</td>\n",
       "      <td>['25444452457', '49949090130', '84860884417', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "0  85131139456  Microencapsulated basil oil (Ocimum basilicum ...   \n",
       "1  85121351780  Perceptions Regarding the SARS-CoV-2 Pandemic'...   \n",
       "2  85131660961  Construction of a short version of the Montrea...   \n",
       "3  85124670542  The effect of strain and pressure on the elect...   \n",
       "4  85143878806  Dynamic Cardiopulmonary and Metabolic Function...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Objective: Microencapsulation is a technique t...   \n",
       "1  Background: The SARS-CoV-2 (COVID-19) pandemic...   \n",
       "2  Background: The Montreal Cognitive Assessment ...   \n",
       "3  Different theoretical methodologies are employ...   \n",
       "4  The purpose of this study was to investigate a...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Sureerat Thuekeaw, Kris Angkanaporn, Chackrit ...   \n",
       "1  Abhijit V. Lele, Sarah Wahlster, Bhunyawee Alu...   \n",
       "2  Solaphat Hemrungrojn, Arisara Amrapala, Michae...   \n",
       "3  Erik Johansson, Ferenc Tasnádi, Annop Ektarawo...   \n",
       "4        Kunanya Masodsai, Rungchai Chaunchaiyakul,    \n",
       "\n",
       "                                            category  year  source  \\\n",
       "0  [('Food Science', '1106', 'AGRI'), ('Physiolog...  2022  scopus   \n",
       "1  [('Surgery', '2746', 'MEDI'), ('Neurology (cli...  2022  scopus   \n",
       "2           [('Neuroscience (all)', '2800', 'NEUR')]  2022  scopus   \n",
       "3  [('Physics and Astronomy (all)', '3100', 'PHYS')]  2022  scopus   \n",
       "4         [('Physiology (medical)', '2737', 'MEDI')]  2022  scopus   \n",
       "\n",
       "                                          references  \n",
       "0  ['85039040394', '85050697915', '84920164411', ...  \n",
       "1  ['85104589379', '85083241171', '85078262578', ...  \n",
       "2  ['84982975791', '84871671961', '85097597113', ...  \n",
       "3  ['0035282206', '0035508561', '0035858409', '18...  \n",
       "4  ['25444452457', '49949090130', '84860884417', ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df = pd.concat([scopus_df, arxiv_df], ignore_index=True)\n",
    "# re-arrange columns\n",
    "combine_df = combine_df[[\"id\", 'title', 'abstract', 'authors', 'category', 'year', 'source', 'references']]\n",
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    888907.000000\n",
       "mean        137.417608\n",
       "std          62.787405\n",
       "min           1.000000\n",
       "25%          90.000000\n",
       "50%         129.000000\n",
       "75%         177.000000\n",
       "max        1194.000000\n",
       "Name: abstract, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the length of abstract\n",
    "combine_df[\"abstract\"].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df[\"id\"] = combine_df[\"id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df.to_csv(\"combined_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qdrant database usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.database.qdrant import QdrantVectorDB\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from src.embedding_model import GeminiEmbedding\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='DSDE-project-embedding')])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = GeminiEmbedding()\n",
    "qdrant_client = QdrantVectorDB(url=\"http://localhost:6333\", embedding_model=embedding_model)\n",
    "qdrant_client.get_collection_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id=667298, version=10426, score=0.7149379, payload={'id': '2410.18541', 'title': 'On Explaining with Attention Matrices', 'source': 'arxiv', 'content': 'This paper explores the much discussed, possible explanatory link between\\nattention weights (AW) in transformer models and predicted output. Contrary to\\nintuition and early research on attention, more recent prior research has\\nprovided formal arguments and empirical evidence that AW are not explanatorily\\nrelevant. We show that the formal arguments are incorrect. We introduce and\\neffectively compute efficient attention, which isolates the effective\\ncomponents of attention matrices in tasks and models in which AW play an\\nexplanatory role. We show that efficient attention has a causal role (provides\\nminimally necessary and sufficient conditions) for predicting model output in\\nNLP tasks requiring contextual information, and we show, contrary to [7], that\\nefficient attention matrices are probability distributions and are effectively\\ncalculable. Thus, they should play an important part in the explanation of\\nattention based model behavior. We offer empirical experiments in support of\\nour method illustrating various properties of efficient attention with various\\nmetrics on four datasets.'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=589838, version=9216, score=0.6909458, payload={'id': 2204.13154, 'title': 'Attention Mechanism in Neural Networks: Where it Comes and Where it Goes', 'source': 'arxiv', 'content': 'A long time ago in the machine learning literature, the idea of incorporating\\na mechanism inspired by the human visual system into neural networks was\\nintroduced. This idea is named the attention mechanism, and it has gone through\\na long development period. Today, many works have been devoted to this idea in\\na variety of tasks. Remarkable performance has recently been demonstrated. The\\ngoal of this paper is to provide an overview from the early work on searching\\nfor ways to implement attention idea with neural networks until the recent\\ntrends. This review emphasizes the important milestones during this progress\\nregarding different tasks. By this way, this study aims to provide a road map\\nfor researchers to explore the current development and get inspired for novel\\napproaches beyond the attention.'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=428417, version=6694, score=0.6898353, payload={'id': 1810.12427, 'title': 'Parallel Attention Mechanisms in Neural Machine Translation', 'source': 'arxiv', 'content': 'Recent papers in neural machine translation have proposed the strict use of\\nattention mechanisms over previous standards such as recurrent and\\nconvolutional neural networks (RNNs and CNNs). We propose that by running\\ntraditionally stacked encoding branches from encoder-decoder attention- focused\\narchitectures in parallel, that even more sequential operations can be removed\\nfrom the model and thereby decrease training time. In particular, we modify the\\nrecently published attention-based architecture called Transformer by Google,\\nby replacing sequential attention modules with parallel ones, reducing the\\namount of training time and substantially improving BLEU scores at the same\\ntime. Experiments over the English to German and English to French translation\\ntasks show that our model establishes a new state of the art.'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=627678, version=9807, score=0.6863031, payload={'id': 2305.07239, 'title': 'T-former: An Efficient Transformer for Image Inpainting', 'source': 'arxiv', 'content': 'Benefiting from powerful convolutional neural networks (CNNs), learning-based\\nimage inpainting methods have made significant breakthroughs over the years.\\nHowever, some nature of CNNs (e.g. local prior, spatially shared parameters)\\nlimit the performance in the face of broken images with diverse and complex\\nforms. Recently, a class of attention-based network architectures, called\\ntransformer, has shown significant performance on natural language processing\\nfields and high-level vision tasks. Compared with CNNs, attention operators are\\nbetter at long-range modeling and have dynamic weights, but their computational\\ncomplexity is quadratic in spatial resolution, and thus less suitable for\\napplications involving higher resolution images, such as image inpainting. In\\nthis paper, we design a novel attention linearly related to the resolution\\naccording to Taylor expansion. And based on this attention, a network called\\n$T$-former is designed for image inpainting. Experiments on several benchmark\\ndatasets demonstrate that our proposed method achieves state-of-the-art\\naccuracy while maintaining a relatively low number of parameters and\\ncomputational complexity. The code can be found at\\n\\\\href{https://github.com/dengyecode/T-former_image_inpainting}{github.com/dengyecode/T-former\\\\_image\\\\_inpainting}'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=586583, version=9165, score=0.6791294, payload={'id': 2203.14263, 'title': 'A General Survey on Attention Mechanisms in Deep Learning', 'source': 'arxiv', 'content': 'Attention is an important mechanism that can be employed for a variety of\\ndeep learning models across many different domains and tasks. This survey\\nprovides an overview of the most important attention mechanisms proposed in the\\nliterature. The various attention mechanisms are explained by means of a\\nframework consisting of a general attention model, uniform notation, and a\\ncomprehensive taxonomy of attention mechanisms. Furthermore, the various\\nmeasures for evaluating attention models are reviewed, and methods to\\ncharacterize the structure of attention models based on the proposed framework\\nare discussed. Last, future work in the field of attention models is\\nconsidered.'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=618899, version=9670, score=0.67432934, payload={'id': 2302.07253, 'title': 'Energy Transformer', 'source': 'arxiv', 'content': 'Our work combines aspects of three promising paradigms in machine learning,\\nnamely, attention mechanism, energy-based models, and associative memory.\\nAttention is the power-house driving modern deep learning successes, but it\\nlacks clear theoretical foundations. Energy-based models allow a principled\\napproach to discriminative and generative tasks, but the design of the energy\\nfunctional is not straightforward. At the same time, Dense Associative Memory\\nmodels or Modern Hopfield Networks have a well-established theoretical\\nfoundation, and allow an intuitive design of the energy function. We propose a\\nnovel architecture, called the Energy Transformer (or ET for short), that uses\\na sequence of attention layers that are purposely designed to minimize a\\nspecifically engineered energy function, which is responsible for representing\\nthe relationships between the tokens. In this work, we introduce the\\ntheoretical foundations of ET, explore its empirical capabilities using the\\nimage completion task, and obtain strong quantitative results on the graph\\nanomaly detection and graph classification tasks.'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=441136, version=6892, score=0.6695216, payload={'id': 1902.02181, 'title': 'Attention in Natural Language Processing', 'source': 'arxiv', 'content': 'Attention is an increasingly popular mechanism used in a wide range of neural\\narchitectures. The mechanism itself has been realized in a variety of formats.\\nHowever, because of the fast-paced advances in this domain, a systematic\\noverview of attention is still missing. In this article, we define a unified\\nmodel for attention architectures in natural language processing, with a focus\\non those designed to work with vector representations of the textual data. We\\npropose a taxonomy of attention models according to four dimensions: the\\nrepresentation of the input, the compatibility function, the distribution\\nfunction, and the multiplicity of the input and/or output. We present the\\nexamples of how prior information can be exploited in attention models and\\ndiscuss ongoing research efforts and open challenges in the area, providing the\\nfirst extensive categorization of the vast body of literature in this exciting\\ndomain.'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=618669, version=9666, score=0.6688921, payload={'id': 2302.06015, 'title': 'A Theoretical Understanding of Shallow Vision Transformers: Learning,\\n  Generalization, and Sample Complexity', 'source': 'arxiv', 'content': 'Vision Transformers (ViTs) with self-attention modules have recently achieved\\ngreat empirical success in many vision tasks. Due to non-convex interactions\\nacross layers, however, theoretical learning and generalization analysis is\\nmostly elusive. Based on a data model characterizing both label-relevant and\\nlabel-irrelevant tokens, this paper provides the first theoretical analysis of\\ntraining a shallow ViT, i.e., one self-attention layer followed by a two-layer\\nperceptron, for a classification task. We characterize the sample complexity to\\nachieve a zero generalization error. Our sample complexity bound is positively\\ncorrelated with the inverse of the fraction of label-relevant tokens, the token\\nnoise level, and the initial model error. We also prove that a training process\\nusing stochastic gradient descent (SGD) leads to a sparse attention map, which\\nis a formal verification of the general intuition about the success of\\nattention. Moreover, this paper indicates that a proper token sparsification\\ncan improve the test performance by removing label-irrelevant and/or noisy\\ntokens, including spurious correlations. Empirical experiments on synthetic\\ndata and CIFAR-10 dataset justify our theoretical results and generalize to\\ndeeper ViTs.'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=524174, version=8190, score=0.6676281, payload={'id': 2010.08242, 'title': 'Unsupervised Extractive Summarization by Pre-training Hierarchical\\n  Transformers', 'source': 'arxiv', 'content': 'Unsupervised extractive document summarization aims to select important\\nsentences from a document without using labeled summaries during training.\\nExisting methods are mostly graph-based with sentences as nodes and edge\\nweights measured by sentence similarities. In this work, we find that\\ntransformer attentions can be used to rank sentences for unsupervised\\nextractive summarization. Specifically, we first pre-train a hierarchical\\ntransformer model using unlabeled documents only. Then we propose a method to\\nrank sentences using sentence-level self-attentions and pre-training\\nobjectives. Experiments on CNN/DailyMail and New York Times datasets show our\\nmodel achieves state-of-the-art performance on unsupervised summarization. We\\nalso find in experiments that our model is less dependent on sentence\\npositions. When using a linear combination of our model and a recent\\nunsupervised model explicitly modeling sentence positions, we obtain even\\nbetter results.'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=537804, version=8403, score=0.6666144, payload={'id': 2101.10927, 'title': 'Attention Can Reflect Syntactic Structure (If You Let It)', 'source': 'arxiv', 'content': 'Since the popularization of the Transformer as a general-purpose feature\\nencoder for NLP, many studies have attempted to decode linguistic structure\\nfrom its novel multi-head attention mechanism. However, much of such work\\nfocused almost exclusively on English -- a language with rigid word order and a\\nlack of inflectional morphology. In this study, we present decoding experiments\\nfor multilingual BERT across 18 languages in order to test the generalizability\\nof the claim that dependency syntax is reflected in attention patterns. We show\\nthat full trees can be decoded above baseline accuracy from single attention\\nheads, and that individual relations are often tracked by the same heads across\\nlanguages. Furthermore, in an attempt to address recent debates about the\\nstatus of attention as an explanatory mechanism, we experiment with fine-tuning\\nmBERT on a supervised parsing objective while freezing different series of\\nparameters. Interestingly, in steering the objective to learn explicit\\nlinguistic structure, we find much of the same structure represented in the\\nresulting attention patterns, with interesting differences with respect to\\nwhich parameters are frozen.'}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_name = \"DSDE-project-embedding\"\n",
    "query = \"I want to know paper about attention is all you need anything paper about transformer in computer science\"\n",
    "search_result = qdrant_client.get_search_results(collection_name=collection_name, query=query, top_k=10)\n",
    "\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2410.18541',\n",
       " 'title': 'On Explaining with Attention Matrices',\n",
       " 'source': 'arxiv',\n",
       " 'content': 'This paper explores the much discussed, possible explanatory link between\\nattention weights (AW) in transformer models and predicted output. Contrary to\\nintuition and early research on attention, more recent prior research has\\nprovided formal arguments and empirical evidence that AW are not explanatorily\\nrelevant. We show that the formal arguments are incorrect. We introduce and\\neffectively compute efficient attention, which isolates the effective\\ncomponents of attention matrices in tasks and models in which AW play an\\nexplanatory role. We show that efficient attention has a causal role (provides\\nminimally necessary and sufficient conditions) for predicting model output in\\nNLP tasks requiring contextual information, and we show, contrary to [7], that\\nefficient attention matrices are probability distributions and are effectively\\ncalculable. Thus, they should play an important part in the explanation of\\nattention based model behavior. We offer empirical experiments in support of\\nour method illustrating various properties of efficient attention with various\\nmetrics on four datasets.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result[0].payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import ScoredPoint\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from app.models.GraphData import Node, NodeType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r2/133yy5hj2g7fq987s7zh0_q40000gn/T/ipykernel_14875/4257178876.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"combined_data.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>year</th>\n",
       "      <th>source</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85131139456.0</td>\n",
       "      <td>Microencapsulated basil oil (Ocimum basilicum ...</td>\n",
       "      <td>Objective: Microencapsulation is a technique t...</td>\n",
       "      <td>Sureerat Thuekeaw, Kris Angkanaporn, Chackrit ...</td>\n",
       "      <td>[('Food Science', '1106', 'AGRI'), ('Physiolog...</td>\n",
       "      <td>2022</td>\n",
       "      <td>scopus</td>\n",
       "      <td>['85039040394', '85050697915', '84920164411', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85121351780.0</td>\n",
       "      <td>Perceptions Regarding the SARS-CoV-2 Pandemic'...</td>\n",
       "      <td>Background: The SARS-CoV-2 (COVID-19) pandemic...</td>\n",
       "      <td>Abhijit V. Lele, Sarah Wahlster, Bhunyawee Alu...</td>\n",
       "      <td>[('Surgery', '2746', 'MEDI'), ('Neurology (cli...</td>\n",
       "      <td>2022</td>\n",
       "      <td>scopus</td>\n",
       "      <td>['85104589379', '85083241171', '85078262578', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85131660961.0</td>\n",
       "      <td>Construction of a short version of the Montrea...</td>\n",
       "      <td>Background: The Montreal Cognitive Assessment ...</td>\n",
       "      <td>Solaphat Hemrungrojn, Arisara Amrapala, Michae...</td>\n",
       "      <td>[('Neuroscience (all)', '2800', 'NEUR')]</td>\n",
       "      <td>2022</td>\n",
       "      <td>scopus</td>\n",
       "      <td>['84982975791', '84871671961', '85097597113', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85124670542.0</td>\n",
       "      <td>The effect of strain and pressure on the elect...</td>\n",
       "      <td>Different theoretical methodologies are employ...</td>\n",
       "      <td>Erik Johansson, Ferenc Tasnádi, Annop Ektarawo...</td>\n",
       "      <td>[('Physics and Astronomy (all)', '3100', 'PHYS')]</td>\n",
       "      <td>2022</td>\n",
       "      <td>scopus</td>\n",
       "      <td>['0035282206', '0035508561', '0035858409', '18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85143878806.0</td>\n",
       "      <td>Dynamic Cardiopulmonary and Metabolic Function...</td>\n",
       "      <td>The purpose of this study was to investigate a...</td>\n",
       "      <td>Kunanya Masodsai, Rungchai Chaunchaiyakul,</td>\n",
       "      <td>[('Physiology (medical)', '2737', 'MEDI')]</td>\n",
       "      <td>2022</td>\n",
       "      <td>scopus</td>\n",
       "      <td>['25444452457', '49949090130', '84860884417', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "0  85131139456.0  Microencapsulated basil oil (Ocimum basilicum ...   \n",
       "1  85121351780.0  Perceptions Regarding the SARS-CoV-2 Pandemic'...   \n",
       "2  85131660961.0  Construction of a short version of the Montrea...   \n",
       "3  85124670542.0  The effect of strain and pressure on the elect...   \n",
       "4  85143878806.0  Dynamic Cardiopulmonary and Metabolic Function...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Objective: Microencapsulation is a technique t...   \n",
       "1  Background: The SARS-CoV-2 (COVID-19) pandemic...   \n",
       "2  Background: The Montreal Cognitive Assessment ...   \n",
       "3  Different theoretical methodologies are employ...   \n",
       "4  The purpose of this study was to investigate a...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Sureerat Thuekeaw, Kris Angkanaporn, Chackrit ...   \n",
       "1  Abhijit V. Lele, Sarah Wahlster, Bhunyawee Alu...   \n",
       "2  Solaphat Hemrungrojn, Arisara Amrapala, Michae...   \n",
       "3  Erik Johansson, Ferenc Tasnádi, Annop Ektarawo...   \n",
       "4        Kunanya Masodsai, Rungchai Chaunchaiyakul,    \n",
       "\n",
       "                                            category  year  source  \\\n",
       "0  [('Food Science', '1106', 'AGRI'), ('Physiolog...  2022  scopus   \n",
       "1  [('Surgery', '2746', 'MEDI'), ('Neurology (cli...  2022  scopus   \n",
       "2           [('Neuroscience (all)', '2800', 'NEUR')]  2022  scopus   \n",
       "3  [('Physics and Astronomy (all)', '3100', 'PHYS')]  2022  scopus   \n",
       "4         [('Physiology (medical)', '2737', 'MEDI')]  2022  scopus   \n",
       "\n",
       "                                          references  \n",
       "0  ['85039040394', '85050697915', '84920164411', ...  \n",
       "1  ['85104589379', '85083241171', '85078262578', ...  \n",
       "2  ['84982975791', '84871671961', '85097597113', ...  \n",
       "3  ['0035282206', '0035508561', '0035858409', '18...  \n",
       "4  ['25444452457', '49949090130', '84860884417', ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"combined_data.csv\")\n",
    "df[\"id\"] = df[\"id\"].astype(str)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_info(search_result: List[ScoredPoint]):\n",
    "    paper_info = {}\n",
    "    for point in search_result:\n",
    "        identifier = str(point.payload.get(\"id\")) + '_' + point.payload.get(\"source\")\n",
    "        if identifier in paper_info:\n",
    "            continue\n",
    "        paper_id = str(point.payload.get(\"id\"))\n",
    "        paper = df[df[\"id\"] == paper_id].to_dict(orient=\"records\")\n",
    "\n",
    "        if len(paper) == 0:\n",
    "            continue\n",
    "\n",
    "        paper_node = Node(\n",
    "            id=paper[0].get(\"id\"),\n",
    "            title=paper[0].get(\"title\"),\n",
    "            type=NodeType.paper,\n",
    "            year=int(paper[0].get(\"year\")),\n",
    "            abstract=paper[0].get(\"abstract\"),\n",
    "            authors=paper[0].get(\"authors\").split(\",\"),\n",
    "            source=paper[0].get(\"source\"), \n",
    "        )\n",
    "\n",
    "        paper_info[identifier] = paper_node\n",
    "    \n",
    "    return list(paper_info.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Node(id='2410.18541', title='On Explaining with Attention Matrices', type=<NodeType.paper: 'paper'>, year=2024, abstract='  This paper explores the much discussed, possible explanatory link between\\nattention weights (AW) in transformer models and predicted output. Contrary to\\nintuition and early research on attention, more recent prior research has\\nprovided formal arguments and empirical evidence that AW are not explanatorily\\nrelevant. We show that the formal arguments are incorrect. We introduce and\\neffectively compute efficient attention, which isolates the effective\\ncomponents of attention matrices in tasks and models in which AW play an\\nexplanatory role. We show that efficient attention has a causal role (provides\\nminimally necessary and sufficient conditions) for predicting model output in\\nNLP tasks requiring contextual information, and we show, contrary to [7], that\\nefficient attention matrices are probability distributions and are effectively\\ncalculable. Thus, they should play an important part in the explanation of\\nattention based model behavior. We offer empirical experiments in support of\\nour method illustrating various properties of efficient attention with various\\nmetrics on four datasets.\\n', authors=['Omar Naim and Nicholas Asher'], source='arxiv'),\n",
       " Node(id='2204.13154', title='Attention Mechanism in Neural Networks: Where it Comes and Where it Goes', type=<NodeType.paper: 'paper'>, year=2022, abstract='  A long time ago in the machine learning literature, the idea of incorporating\\na mechanism inspired by the human visual system into neural networks was\\nintroduced. This idea is named the attention mechanism, and it has gone through\\na long development period. Today, many works have been devoted to this idea in\\na variety of tasks. Remarkable performance has recently been demonstrated. The\\ngoal of this paper is to provide an overview from the early work on searching\\nfor ways to implement attention idea with neural networks until the recent\\ntrends. This review emphasizes the important milestones during this progress\\nregarding different tasks. By this way, this study aims to provide a road map\\nfor researchers to explore the current development and get inspired for novel\\napproaches beyond the attention.\\n', authors=['Derya Soydaner'], source='arxiv'),\n",
       " Node(id='1810.12427', title='Parallel Attention Mechanisms in Neural Machine Translation', type=<NodeType.paper: 'paper'>, year=2018, abstract='  Recent papers in neural machine translation have proposed the strict use of\\nattention mechanisms over previous standards such as recurrent and\\nconvolutional neural networks (RNNs and CNNs). We propose that by running\\ntraditionally stacked encoding branches from encoder-decoder attention- focused\\narchitectures in parallel, that even more sequential operations can be removed\\nfrom the model and thereby decrease training time. In particular, we modify the\\nrecently published attention-based architecture called Transformer by Google,\\nby replacing sequential attention modules with parallel ones, reducing the\\namount of training time and substantially improving BLEU scores at the same\\ntime. Experiments over the English to German and English to French translation\\ntasks show that our model establishes a new state of the art.\\n', authors=['Julian Richard Medina', ' Jugal Kalita'], source='arxiv'),\n",
       " Node(id='2305.07239', title='T-former: An Efficient Transformer for Image Inpainting', type=<NodeType.paper: 'paper'>, year=2023, abstract='  Benefiting from powerful convolutional neural networks (CNNs), learning-based\\nimage inpainting methods have made significant breakthroughs over the years.\\nHowever, some nature of CNNs (e.g. local prior, spatially shared parameters)\\nlimit the performance in the face of broken images with diverse and complex\\nforms. Recently, a class of attention-based network architectures, called\\ntransformer, has shown significant performance on natural language processing\\nfields and high-level vision tasks. Compared with CNNs, attention operators are\\nbetter at long-range modeling and have dynamic weights, but their computational\\ncomplexity is quadratic in spatial resolution, and thus less suitable for\\napplications involving higher resolution images, such as image inpainting. In\\nthis paper, we design a novel attention linearly related to the resolution\\naccording to Taylor expansion. And based on this attention, a network called\\n$T$-former is designed for image inpainting. Experiments on several benchmark\\ndatasets demonstrate that our proposed method achieves state-of-the-art\\naccuracy while maintaining a relatively low number of parameters and\\ncomputational complexity. The code can be found at\\n\\\\href{https://github.com/dengyecode/T-former_image_inpainting}{github.com/dengyecode/T-former\\\\_image\\\\_inpainting}\\n', authors=['Ye Deng', ' Siqi Hui', ' Sanping Zhou', ' Deyu Meng', ' Jinjun Wang'], source='arxiv'),\n",
       " Node(id='2203.14263', title='A General Survey on Attention Mechanisms in Deep Learning', type=<NodeType.paper: 'paper'>, year=2022, abstract='  Attention is an important mechanism that can be employed for a variety of\\ndeep learning models across many different domains and tasks. This survey\\nprovides an overview of the most important attention mechanisms proposed in the\\nliterature. The various attention mechanisms are explained by means of a\\nframework consisting of a general attention model, uniform notation, and a\\ncomprehensive taxonomy of attention mechanisms. Furthermore, the various\\nmeasures for evaluating attention models are reviewed, and methods to\\ncharacterize the structure of attention models based on the proposed framework\\nare discussed. Last, future work in the field of attention models is\\nconsidered.\\n', authors=['Gianni Brauwers and Flavius Frasincar'], source='arxiv'),\n",
       " Node(id='2302.07253', title='Energy Transformer', type=<NodeType.paper: 'paper'>, year=2023, abstract='  Our work combines aspects of three promising paradigms in machine learning,\\nnamely, attention mechanism, energy-based models, and associative memory.\\nAttention is the power-house driving modern deep learning successes, but it\\nlacks clear theoretical foundations. Energy-based models allow a principled\\napproach to discriminative and generative tasks, but the design of the energy\\nfunctional is not straightforward. At the same time, Dense Associative Memory\\nmodels or Modern Hopfield Networks have a well-established theoretical\\nfoundation, and allow an intuitive design of the energy function. We propose a\\nnovel architecture, called the Energy Transformer (or ET for short), that uses\\na sequence of attention layers that are purposely designed to minimize a\\nspecifically engineered energy function, which is responsible for representing\\nthe relationships between the tokens. In this work, we introduce the\\ntheoretical foundations of ET, explore its empirical capabilities using the\\nimage completion task, and obtain strong quantitative results on the graph\\nanomaly detection and graph classification tasks.\\n', authors=['Benjamin Hoover', ' Yuchen Liang', ' Bao Pham', ' Rameswar Panda', ' Hendrik\\n  Strobelt', ' Duen Horng Chau', ' Mohammed J. Zaki', ' Dmitry Krotov'], source='arxiv'),\n",
       " Node(id='1902.02181', title='Attention in Natural Language Processing', type=<NodeType.paper: 'paper'>, year=2021, abstract='  Attention is an increasingly popular mechanism used in a wide range of neural\\narchitectures. The mechanism itself has been realized in a variety of formats.\\nHowever, because of the fast-paced advances in this domain, a systematic\\noverview of attention is still missing. In this article, we define a unified\\nmodel for attention architectures in natural language processing, with a focus\\non those designed to work with vector representations of the textual data. We\\npropose a taxonomy of attention models according to four dimensions: the\\nrepresentation of the input, the compatibility function, the distribution\\nfunction, and the multiplicity of the input and/or output. We present the\\nexamples of how prior information can be exploited in attention models and\\ndiscuss ongoing research efforts and open challenges in the area, providing the\\nfirst extensive categorization of the vast body of literature in this exciting\\ndomain.\\n', authors=['Andrea Galassi', ' Marco Lippi', ' Paolo Torroni'], source='arxiv'),\n",
       " Node(id='2302.06015', title='A Theoretical Understanding of Shallow Vision Transformers: Learning,\\n  Generalization, and Sample Complexity', type=<NodeType.paper: 'paper'>, year=2023, abstract='  Vision Transformers (ViTs) with self-attention modules have recently achieved\\ngreat empirical success in many vision tasks. Due to non-convex interactions\\nacross layers, however, theoretical learning and generalization analysis is\\nmostly elusive. Based on a data model characterizing both label-relevant and\\nlabel-irrelevant tokens, this paper provides the first theoretical analysis of\\ntraining a shallow ViT, i.e., one self-attention layer followed by a two-layer\\nperceptron, for a classification task. We characterize the sample complexity to\\nachieve a zero generalization error. Our sample complexity bound is positively\\ncorrelated with the inverse of the fraction of label-relevant tokens, the token\\nnoise level, and the initial model error. We also prove that a training process\\nusing stochastic gradient descent (SGD) leads to a sparse attention map, which\\nis a formal verification of the general intuition about the success of\\nattention. Moreover, this paper indicates that a proper token sparsification\\ncan improve the test performance by removing label-irrelevant and/or noisy\\ntokens, including spurious correlations. Empirical experiments on synthetic\\ndata and CIFAR-10 dataset justify our theoretical results and generalize to\\ndeeper ViTs.\\n', authors=['Hongkang Li', ' Meng Wang', ' Sijia Liu', ' Pin-yu Chen'], source='arxiv'),\n",
       " Node(id='2010.08242', title='Unsupervised Extractive Summarization by Pre-training Hierarchical\\n  Transformers', type=<NodeType.paper: 'paper'>, year=2021, abstract='  Unsupervised extractive document summarization aims to select important\\nsentences from a document without using labeled summaries during training.\\nExisting methods are mostly graph-based with sentences as nodes and edge\\nweights measured by sentence similarities. In this work, we find that\\ntransformer attentions can be used to rank sentences for unsupervised\\nextractive summarization. Specifically, we first pre-train a hierarchical\\ntransformer model using unlabeled documents only. Then we propose a method to\\nrank sentences using sentence-level self-attentions and pre-training\\nobjectives. Experiments on CNN/DailyMail and New York Times datasets show our\\nmodel achieves state-of-the-art performance on unsupervised summarization. We\\nalso find in experiments that our model is less dependent on sentence\\npositions. When using a linear combination of our model and a recent\\nunsupervised model explicitly modeling sentence positions, we obtain even\\nbetter results.\\n', authors=['Shusheng Xu', ' Xingxing Zhang', ' Yi Wu', ' Furu Wei and Ming Zhou'], source='arxiv'),\n",
       " Node(id='2101.10927', title='Attention Can Reflect Syntactic Structure (If You Let It)', type=<NodeType.paper: 'paper'>, year=2021, abstract='  Since the popularization of the Transformer as a general-purpose feature\\nencoder for NLP, many studies have attempted to decode linguistic structure\\nfrom its novel multi-head attention mechanism. However, much of such work\\nfocused almost exclusively on English -- a language with rigid word order and a\\nlack of inflectional morphology. In this study, we present decoding experiments\\nfor multilingual BERT across 18 languages in order to test the generalizability\\nof the claim that dependency syntax is reflected in attention patterns. We show\\nthat full trees can be decoded above baseline accuracy from single attention\\nheads, and that individual relations are often tracked by the same heads across\\nlanguages. Furthermore, in an attempt to address recent debates about the\\nstatus of attention as an explanatory mechanism, we experiment with fine-tuning\\nmBERT on a supervised parsing objective while freezing different series of\\nparameters. Interestingly, in steering the objective to learn explicit\\nlinguistic structure, we find much of the same structure represented in the\\nresulting attention patterns, with interesting differences with respect to\\nwhich parameters are frozen.\\n', authors=['Vinit Ravishankar', ' Artur Kulmizev', ' Mostafa Abdou', ' Anders S{\\\\o}gaard', '\\n  Joakim Nivre'], source='arxiv')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_paper_info(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
